# NER & Actor Pipeline: Technical Documentation

Этот документ описывает полную архитектуру системы извлечения, обработки и канонизации именованных сущностей (NER) и Акторов в проекте.

## 1. Обзор Архитектуры

Система реализует гибридный подход, комбинируя скорость локальных моделей (spaCy) с точностью и гибкостью LLM (Gemini).

### Основные компоненты:
1.  **HybridNERService**: Оркестратор извлечения (Extraction). Решает, какой инструмент использовать (spaCy или LLM) для конкретного текста.
2.  **ActorCanonicalizationService**: Сервис нормализации (Processing). Отвечает за приведение имен к единому стандарту, лемматизацию и обогащение через Wikidata.
3.  **WikidataService**: Интерфейс к внешнему Knowledge Graph для получения QID, канонических имен и метаданных.
4.  **ActorsExtractionService**: Высокоуровневый сервис, управляющий обновлением графа и дедупликацией.

---

## 2. Алгоритм Извлечения (HybridNERService)

Идея: использовать spaCy как основной быстрый метод, а LLM вызывать только для сложных случаев, перепроверки или отсутствия результатов.

### Этапы:
1.  **Детекция языка**: Определяем наличие кириллицы.
    *   Если кириллица > 30% или есть русские буквы: загружаем `ru_core_news_md` (или `lg`).
    *   Иначе: используем `en_core_web_sm` (или `lg`).
2.  **Первичный проход (spaCy)**:
    *   Извлекаем сущности.
    *   Вычисляем эвристический `confidence` (на основе длины, типа, частоты).
    *   Разделяем на `High Confidence` (≥ 0.75) и `Low Confidence` (< 0.75).
3.  **LLM Layer (Gemini)**:
    *   Вызывается, если:
        *   Найдено мало сущностей (< 3).
        *   Есть `Low Confidence` сущности (для перепроверки).
        *   spaCy недоступен.
    *   Задачи LLM: подтвердить сомнительные, найти пропущенные, исправить границы сущностей.

**Метрики (по тестам):**
*   F1-Score: ~89-90%
*   Экономия токенов LLM: до 70-80% по сравнению с чистым LLM подходом.

---

## 3. Канонизация и Лемматизация (ActorCanonicalizationService)

Обрабатывает "сырые" строки, полученные от NER, превращая их в структурированные данные.

### Пайплайн обработки:
1.  **Лемматизация (Russian)**:
    *   Использует spaCy для приведения слов в начальную форму (Им. падеж).
    *   Пример: "Украиной" -> "Украина", "Трампом" -> "Трамп".
2.  **Поиск в Wikidata**:
    *   Ищем сущность по лемматизированному имени.
    *   Получаем `Wikidata QID` (уникальный идентификатор, например, `Q212` для Украины).
3.  **Приоритет Английского**:
    *   Независимо от языка поиска, в качестве `canonical_name` сохраняется **английское название** из Wikidata (если есть).
    *   Это гарантирует, что "Russia", "Россия" и "Russie" схлопнутся в один узел `Russia`.
4.  **Алиасы**:
    *   Оригинальное написание сохраняется в список `aliases` (с типом `original`).
    *   Загружаются дополнительные алиасы из Wikidata.

---

## 4. Дедупликация и Граф (ActorsExtractionService)

Отвечает за целостность графа при добавлении новых данных.

### Логика слияния (`deduplicate_actors`):
Рефакторинг разделил логику на чистые этапы:
1.  **Группировка по QID**: Самый надежный метод. Если у двух акторов одинаковый `wikidata_qid` -> это один актор.
2.  **Группировка по Имени**: Вторичный метод для акторов без QID (нормализация строк, удаление спецсимволов).
3.  **Слияние (Merging)**:
    *   Выбирается "Target" актор (приоритет: есть QID, более полные данные).
    *   Все "Source" акторы вливаются в него:
        *   Алиасы объединяются.
        *   Метаданные обогащаются.
        *   Ссылки в Новостях и Историях обновляются на ID целевого актора.
        *   Старые узлы удаляются из графа.

---

## 5. Установка и Настройка

### Требования
*   Python 3.9+
*   `spacy`
*   Модели:
    ```bash
    python -m spacy download en_core_web_sm
    python -m spacy download ru_core_news_md
    ```

### Конфигурация (.env)
```ini
SPACY_MODEL=en_core_web_sm  # Дефолтная модель
WIKIDATA_ENABLED=true       # Включить запросы к Wikidata
LLM_PROVIDER=gemini         # Провайдер LLM
```

### Запуск тестов
```bash
# Тесты пайплайна логики
python -m pytest tests/test_actor_pipeline_logic.py

# Тесты интеграции с Wikidata
python -m pytest tests/test_wikidata_service.py
```

